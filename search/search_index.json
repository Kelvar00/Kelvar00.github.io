{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u4e3b\u9875","text":"<p>kelvar\u7684\u5c0f\u7ad9\uff0c\u65bd\u5de5\u4e2d\uff08\u771f\u7684\u6709\u4e00\u5929\u80fd\u65bd\u5de5\u51fa\u6765\u5417\uff09</p>"},{"location":"notebook/test/","title":"intro","text":"<p>\u4e0d\u77e5\u9053\u4ee5\u540e\u8fd9\u91cc\u4f1a\u4e0d\u4f1a\u6709\u4e00\u4e9b\u4e1c\u897f...</p>"},{"location":"notebook/paper_note/bayesian_iclnote/","title":"Bayesian model of dynamic image stabilization in the visual system","text":"<p>PNAS: Bayesian model of dynamic image stabilization in the visual system</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#author","title":"author","text":"<p>Yoram Burak Uri Rokni Markus Meister Haim Somplolinsky</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#purpose","title":"purpose","text":"<p>To draw a model to explain why human visual system can stablize dynamic image. </p>"},{"location":"notebook/paper_note/bayesian_iclnote/#pipline","title":"pipline","text":"<p>stage 1 As the paper proved, when  performing high acuity visual tasks, the brain must consider the drift of image.</p> <p>stage 2 draw a decode strategy to interprete the spikes emitted by the retina(\u89c6\u7f51\u819c) difficulty: possible stimuli has an exponentially large number</p> <p>stage 3 Another implementation involves two populations of cells, one tracks position, another represents a stabilized estimate  of the image itself.</p> <p>the spikes from the retina are dynamically routed to the two populations and are interpreted in a probablistic mannner.</p> <p>The paper also considers the architecture of neural circuitry, measures the performance of human fixational eye motion (under measured statistics)</p> <p>prediction In high auity tasks, fixed features within the visual scene are beneficial because they provide information about the drifting position of the image.</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#preceding-work","title":"preceding work","text":"<p>brain infers surroundings (Hypotheses)</p> <p>when noisy, the interpretation becomes ambiguous, and those hypotheses compete.</p> <p>How could the brain estimate the image from 2D scenes</p> <p>sources of ambiguity: 1. noise in neural circuitry 2. random movements of the eye that leads to image jitter on the retina</p> <p>An ideal Bayesian decoder in the brain take those into consider. But the number of variables we must consider is too large.</p> <p>Prior work on Bayesian inference focused on simplified conditions in which the subject estimates only a single typicallly static sensory variable.</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#model","title":"model","text":"<p>simulated cells(retina) The researchers model the fovea as a homogeneous array of the retinal ganglion cells of a single type, arranged on a rectangular grid.</p> <p>input image The image consist of black-and-white pixels on the same grid, whose itensities are drawn independently from a binary distribution.</p> <p>fire process(output) The firing of each cell is an inhomogeneous Poisson process(\u975e\u9f50\u6b21\u6cca\u677e\u8fc7\u7a0b), whose rate depends on the image pixel in the receptive field</p> <p>a simple version pixel on \u2192 cell fire rate = \\(\\lambda 1\\)  pixel off \u2192 cell fire rate = \\(\\lambda 0\\) </p> <p>a realistic version later the fire rate dependas on the past light intensity within the retina's integration time.</p> <p>about fixational movements the fixational movements of the image over the retina are modeled as a discrete random walk</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#task-visualization","title":"task visualization","text":"<p>Use 'E' as an input\uff0c apply fixational movements on the image. The fig C refers to the spikes generated by their model retina. In which \\(\\lambda_0 =10Hz\\),\\(\\lambda_1 = 100Hz\\) [figure]</p> <p>Indeed, images of a Snellen letter derived from simple spike accumulation in each pixel seem almost random. And so without some knowledge of the image trajectory, such a reconstruction is impossible.</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#math-theory-factorized-bayesian-decoder","title":"math theory -- factorized bayesian decoder","text":""},{"location":"notebook/paper_note/bayesian_iclnote/#definition","title":"definition","text":"<p>\\(s\\): a probabilistic estimate of the image \\(x\\): retinal position \\(p(x,t)\\): probability distribution of positions \\(p_i (s_i, t)\\): probability distributions for individual pixels in the stabilized coordinates of the image \\(s_i\\): a pixel is on or off (1/0) \\(m_i (t) = p_i (1,t) = 1- p_i (0,t)\\) \\(D\\) : D \u2243 100 arcmin2/s, diffusion coefficient(a human feature, )</p> <p>The full Bayesian estimate is like:</p> \\[p(s,x,t) = p(x,t) \\prod \\limits_i p_i (s_i,t)\\] <p>This form ignores any correlations between the values of different pixels or between the image and its position.</p> <p>Meaning that each pixel is considered desparately. And the relation between the position and the content of the picture. </p>"},{"location":"notebook/paper_note/bayesian_iclnote/#gradient","title":"gradient","text":"<p>\u5206\u6bb5\u51fd\u6570 between spike</p> \\[\\frac{\\partial p(x,t)}{\\partial t} = D \\nabla^2 p(x,t) \\] \\[\\nabla^2 p(x,t) = p(\\overline{x}, t) - p(x,t) = \\sum_{x' \\in NN(x)}{p(x',t)} - 4p(x,t)\\] <p>\\(D\\)\u662f\u7cfb\u6570</p> <p>\u53c8</p> \\[\\frac{\\partial m_i (t)}{\\partial t} = -\\Delta \\lambda[1-m_i (t)]m_i (t)\\] \\[\\Delta \\lambda = \\lambda_1 - \\lambda_0\\] <ol> <li>\\(m_i (t)\\) decays toward zero in the absence of spikes, with a rate proportional to \\(\\Delta \\lambda\\)</li> <li>if \\(m_i\\) is either 0 or 1, such that the decoder is certain about the value of  pixel \\(i, m_i\\) remains unchanged</li> </ol> <p>due to spike At time t, the ganglion cell k fires a spike.</p> \\[p(x,t_+) \\propto [\\lambda_0 + \\Delta \\lambda m_{k-x} (t_-)] \\cdot p(x,t_-)\\] \\[m_i (t_+) = m_i (t_-) + \\Phi [m_i (t_-)]\\cdot p (k-i,t_+)\\] \\[\\Phi(m) = \\frac{\\Delta \\lambda m(1-m)}{\\lambda_0 + \\Delta \\lambda m }\\] <p>\u2192 The change in \\(m_i\\) is proportional to the estimated probability that the image is at position \\(k-i\\).</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#network-implementation","title":"network implementation","text":"<p>RGC: retina ganglion cell This result suggests a network architecture with two divergent projections from retinal ganglion cells to the what cells and the where cells, along with reciprocal recurrent connections between both of these populations. [figure 1-D]</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#performance","title":"performance","text":"<p>Comparison between the model and static decoder.</p> <p>The response of factorized decoder to a sample stimulus is illustrated in  [Figure 2-A]</p> <p>The estimate of the image itself, represented by activity in the what population, gradually improves with time. In this example almost all of the pixels are estimated correctly at 300 ms</p> <p>[Figure 2-B]</p> <p>When tested with many random images, the factorized decoder routinely reconstructed 90% of the pixels correctly in just 100 ms (Fig. 2B). By comparison, a static decoder that ignores eye movements and simply accumulates spikes performed very poorly: Shortly after stimulus onset it reached a maximum of nearly 60% correctly estimated pixels, but then the blurring from retinal motion took its toll.</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#trend","title":"trend","text":"<p>Performance improves with slower eye movements, higher firing rates, and larger image size</p> <p>When D is small, the decoder easily tracks the position of the image, and performance is limited only by the stochasticity of the ganglion cell response. As D increases, the performance degrades due to uncertainty about the position (Fig. 3A). The convergence time increases sharply above a critical value of D. This value is proportional to the RGC \ufb01ring rates, as can be deduced from dimensional analysis. With a larger image, more information is available about the trajectory, and the decoder\u2019s performance improves markedly (Fig. 3B). Further analysis shows that increasing the number of pixels by a factor f acts roughly like a reduction of D by a factor \\(\\sqrt{f}\\). This sensitivity to image size should be observable in psychophysical experiments.</p> <p>[figure 3-A]</p> <p>[figure 3-B]</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#simulate-human","title":"simulate human","text":"<p>With D set to 100 \\(arcmin^2 /s\\), corresponding to the measured statistics of human \ufb01xational drift (11\u201313), the factorized decoder performs well on images that cover at least 40 \u00d7 40 pixels (20 \u00d7 20 arcmin) (Fig. 3B). Reconstruction improves dramatically if one is satis\ufb01ed with a lower resolution. For example, if the pixel size is increased from 0.5 to 1 arcmin, then the eye drift changes the pixel contents less rapidly, and four ganglion cells are available to report each pixel. Under these conditions, small 5 \u00d7 5 arcmin images can be decoded rapidly to high accuracy (Fig. 3B).</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#in-more-realistic-scene","title":"in more realistic scene","text":"<p>smaller picture could be better reconstructed</p> <p>but the advantage of the model will be more salient when it comes to more pixels</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#discrimination-task","title":"discrimination task","text":"<p>The possible images represents the letters A-Z. Spikes are generated by a model retina with a biphasic temporal filter and diffusion coeffient D = 100 \\(arcmin^2 /s\\)and fed into decoder.</p> <p>biphasic temporal filter: to minus the affect of noise</p> <p>The decoder achieves a 90% success rate after ~300ms(about the length of a human fixation), compatible with human vision on this task.</p> <p>static: ~50%</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#discussion","title":"Discussion","text":""},{"location":"notebook/paper_note/bayesian_iclnote/#alternative-approaches","title":"alternative approaches","text":"<p>By stabilizing the retinal image, as proposed here, fixational image motion is dealt with once and for all by dedicated neural circuitry that performs the same computation regardless of the image content.</p> <p>This division of labor is functionally attractive, but one can imagine an alternative scenario in which the visual system deals with fixational motion separately whenever it analyzes the foveal image for a specific visual task.</p> <p>The strategy, piecewise static decoder: 1. in each short time window, generate a positioninvariant likelihood that each of the possible letters is in the image, using the static decoder 2. summate these log-likelihoods across windows to accumulate evidence over time, while ignoring the continuity of the trajectory across adjoining windows.</p> <p>can work well in the letter discrimination task</p> <p>drawbacks: 1. seems complicated, because intricate neural circuitry must be set up for each possible pattern and every kind of visual task. 2. two eyes will produce relative jitter 3. not consistent with RGC:      - When the temporal response properties of RGCs are taken into account, eye motion has two competing effects within our model. On one hand, it introduces ambiguity in the interpretation of retinal spikes. On the other hand, it helps drive the RGCs, whose response to completely static stimuli is weak.       - Previous analysis of ideal discrimination between two small stimuli at the limit of visual acuity suggested that a small drift would be beneficial, but the actual eye movements of human subjects are much larger and on balance deleterious </p> <p>Indeed, it seems that certain types of retinal ganglion cells appear designed to ignore global image motion entirely and respond only when an object moves relative to the background scene.</p> <p>A broader question is hwo the brain forms a stable scene representation across saccades(big movements), in which the computational principles presented here may not be useful in brain. In fact, the brain might use a different neural circuity to process those conditions.</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#implement-in-brain","title":"implement in brain","text":"<p>The factorized decoder mentioned above is based on a a hypothesis that the image pixels are the fundamental units.</p> <p>But if the computation is performed in the visual cortex(\u89c6\u795e\u7ecf\u76ae\u5c42), the decoder may represent propabilities for presence of more complex features.</p> <p>The implementation of the factorized decoding strategy has several salient features. 1. divergent afferents, where &lt;&lt; what 2. signal from retina to where&amp;what requires a multiplicative gating controlled in a reciprocal(\u76f8\u4e92\u7684\uff0c\u4ea4\u4e92\u7684) fashion by the signals in those populations. 3. In the where popilstion, local excitatory connections are required to implement the diffusive update between spikes, and a global divisiv mechanism. 4. involve local nonlinerities\uff08\u5c40\u90e8\u975e\u7ebf\u6027\u6027\uff09.</p>"},{"location":"notebook/paper_note/bayesian_iclnote/#location","title":"Location","text":"<p>So where should these circuit be in human visual system?</p> <p>fixational drift(movement) is indepent for two eyes \u2192 the circuit should be within the monocular part of the visual pathway(\u89c6\u89c9\u901a\u8def\u7684\u5355\u773c\u90e8\u5206)</p> <p>If they are fixed on retina \u2192 not accurate enough to perform such task</p> <p>Maybe at foveal region of V1(V1\u533a\u4e2d\u592e\u51f9\u533a\u57df).</p> <p>How to combine the images from two eyes hypothesis: two monocular populations of where neurons that control the inputs to a single population of what neurons</p> <p>Such a binocular representation of the stabilized image may appear in disparity-selective neurons in V1 or downstream of V1, for example in a binocular population in V2 that receives monocular inputs</p> <p>further experiment: record from cortical neurons that represent the primate fovea,whose receptivefield structure is fine enough to resolve patterns close to the animal\u2019s acuity.</p>"},{"location":"tattle/test/","title":"intro","text":"<p>\u4e0d\u77e5\u9053\u4ee5\u540e\u8fd9\u91cc\u4f1a\u4e0d\u4f1a\u6709\u4e00\u4e9b\u4e1c\u897f...</p>"}]}